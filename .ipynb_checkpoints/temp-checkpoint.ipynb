{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a930d300",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec8a5735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the configuration from the JSON file\n",
    "with open(r'C:\\Users\\Rishabh\\Documents\\3d-hcct\\config.json', 'r') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12165daa-ffcd-40c3-aeb6-ac63d37b9ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.gitignore',\n",
       " '.ipynb_checkpoints',\n",
       " 'config.json',\n",
       " 'model.py',\n",
       " 'README.md',\n",
       " 'temp.ipynb',\n",
       " '__pycache__']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "os.listdir(r'C:\\Users\\Rishabh\\Documents\\3d-hcct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b7aa4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ViTForClassfication\n",
    "\n",
    "# Initialize the model with the loaded configuration\n",
    "model = ViTForClassfication(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f94665ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model, trainable_only=True):\n",
    "    if trainable_only:\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7462d77-676d-4a48-98b9-92338d7de1bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'checkpoint_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m checkpoint = torch.load(\u001b[43mcheckpoint_path\u001b[49m, map_location=\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m state_dict = checkpoint[\u001b[33m'\u001b[39m\u001b[33mstate_dict\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Remove 'module.' prefix if it exists\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'checkpoint_path' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "state_dict = checkpoint['state_dict']\n",
    "\n",
    "# Remove 'module.' prefix if it exists\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k.replace('module.', '')  # strip the prefix\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "model.load_state_dict(new_state_dict, strict=True)  # strict ensures all match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29896f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 4715246\n",
      "Trainable parameters: 4715246\n"
     ]
    }
   ],
   "source": [
    "total_params = count_parameters(model, trainable_only=False)\n",
    "trainable = count_parameters(model, trainable_only=True)\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e74c2c58-3417-4e0e-84f2-b6df8f02324c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m img1 = nib.load(os.path.join(DataFolder, Files[indx1]))\n\u001b[32m      7\u001b[39m img2 = nib.load(os.path.join(DataFolder, Files[indx2]))\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m data = \u001b[43mimg\u001b[49m.get_fdata()  \n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mData shape:\u001b[39m\u001b[33m\"\u001b[39m, data.shape)\n\u001b[32m     12\u001b[39m x1 = img1.get_fdata()  \n",
      "\u001b[31mNameError\u001b[39m: name 'img' is not defined"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "DataFolder = r'C:\\Users\\Rishabh\\Documents\\TrimeseData'\n",
    "Files = os.listdir(DataFolder)\n",
    "indx1 = 29\n",
    "indx2 = 30\n",
    "img1 = nib.load(os.path.join(DataFolder, Files[indx1]))\n",
    "img2 = nib.load(os.path.join(DataFolder, Files[indx2]))\n",
    "\n",
    "\n",
    "data = img.get_fdata()  \n",
    "print(\"Data shape:\", data.shape)\n",
    "x1 = img1.get_fdata()  \n",
    "x2 = img2.get_fdata()  \n",
    "x1 = torch.from_numpy(x1)\n",
    "x2 = torch.from_numpy(x2)\n",
    "x1 = x1.unsqueeze(0).unsqueeze(0).float()\n",
    "x2 = x2.unsqueeze(0).unsqueeze(0).float()\n",
    "print(x1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "903f65e1-8f0d-45a5-9ae5-324e877f93da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original x shape:-  torch.Size([1, 1, 91, 109, 91])\n",
      "x1 shape:-  torch.Size([1, 32, 45, 54, 45])\n",
      "x2 shape:-  torch.Size([1, 64, 22, 27, 22])\n",
      "x3 shape:-  torch.Size([1, 128, 11, 13, 11])\n",
      "x4 shape:-  torch.Size([1, 256, 5, 6, 5])\n",
      "x before rearrange:-  torch.Size([1, 512, 2, 3, 2])\n",
      "Final x shape(patch embedding):-  torch.Size([1, 512, 12])\n",
      "cls_tokens:-  torch.Size([1, 1, 12])\n",
      "Embedding shape:-   torch.Size([1, 513, 12])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "torch.Size([513, 513])\n",
      "original x shape:-  torch.Size([1, 1, 91, 109, 91])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rishabh\\anaconda3\\envs\\3d-hcct\\Lib\\site-packages\\torch\\cuda\\__init__.py:182: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11080). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10\\cuda\\CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 shape:-  torch.Size([1, 32, 45, 54, 45])\n",
      "x2 shape:-  torch.Size([1, 64, 22, 27, 22])\n",
      "x3 shape:-  torch.Size([1, 128, 11, 13, 11])\n",
      "x4 shape:-  torch.Size([1, 256, 5, 6, 5])\n",
      "x before rearrange:-  torch.Size([1, 512, 2, 3, 2])\n",
      "Final x shape(patch embedding):-  torch.Size([1, 512, 12])\n",
      "cls_tokens:-  torch.Size([1, 1, 12])\n",
      "Embedding shape:-   torch.Size([1, 513, 12])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "torch.Size([513, 513])\n",
      "original x shape:-  torch.Size([1, 1, 91, 109, 91])\n",
      "x1 shape:-  torch.Size([1, 32, 45, 54, 45])\n",
      "x2 shape:-  torch.Size([1, 64, 22, 27, 22])\n",
      "x3 shape:-  torch.Size([1, 128, 11, 13, 11])\n",
      "x4 shape:-  torch.Size([1, 256, 5, 6, 5])\n",
      "x before rearrange:-  torch.Size([1, 512, 2, 3, 2])\n",
      "Final x shape(patch embedding):-  torch.Size([1, 512, 12])\n",
      "cls_tokens:-  torch.Size([1, 1, 12])\n",
      "Embedding shape:-   torch.Size([1, 513, 12])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "torch.Size([513, 513])\n",
      "original x shape:-  torch.Size([1, 1, 91, 109, 91])\n",
      "x1 shape:-  torch.Size([1, 32, 45, 54, 45])\n",
      "x2 shape:-  torch.Size([1, 64, 22, 27, 22])\n",
      "x3 shape:-  torch.Size([1, 128, 11, 13, 11])\n",
      "x4 shape:-  torch.Size([1, 256, 5, 6, 5])\n",
      "x before rearrange:-  torch.Size([1, 512, 2, 3, 2])\n",
      "Final x shape(patch embedding):-  torch.Size([1, 512, 12])\n",
      "cls_tokens:-  torch.Size([1, 1, 12])\n",
      "Embedding shape:-   torch.Size([1, 513, 12])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "torch.Size([513, 513])\n",
      "original x shape:-  torch.Size([1, 1, 91, 109, 91])\n",
      "x1 shape:-  torch.Size([1, 32, 45, 54, 45])\n",
      "x2 shape:-  torch.Size([1, 64, 22, 27, 22])\n",
      "x3 shape:-  torch.Size([1, 128, 11, 13, 11])\n",
      "x4 shape:-  torch.Size([1, 256, 5, 6, 5])\n",
      "x before rearrange:-  torch.Size([1, 512, 2, 3, 2])\n",
      "Final x shape(patch embedding):-  torch.Size([1, 512, 12])\n",
      "cls_tokens:-  torch.Size([1, 1, 12])\n",
      "Embedding shape:-   torch.Size([1, 513, 12])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n",
      "attention_output:-  torch.Size([1, 513, 1])\n",
      "attention_probs:-  torch.Size([1, 513, 513])\n",
      "query:-  torch.Size([1, 513, 1])\n",
      "key:-  torch.Size([1, 513, 1])\n",
      "value:-  torch.Size([1, 513, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m x1 = x1.unsqueeze(\u001b[32m0\u001b[39m).unsqueeze(\u001b[32m0\u001b[39m).float()\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     logits, all_attentions = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m attn = torch.stack(all_attentions, dim=\u001b[32m0\u001b[39m)\n\u001b[32m     13\u001b[39m attn_mean = attn.mean(dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\3d-hcct\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\3d-hcct\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\3d-hcct\\model.py:348\u001b[39m, in \u001b[36mViTForClassfication.forward\u001b[39m\u001b[34m(self, x, output_attentions)\u001b[39m\n\u001b[32m    345\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, output_attentions=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    346\u001b[39m     embedding_output = \u001b[38;5;28mself\u001b[39m.embedding(x)\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m     encoder_output, all_attentions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    349\u001b[39m     cls_logits, activation_logits = encoder_output[:, \u001b[32m0\u001b[39m, :], encoder_output[:, \u001b[32m1\u001b[39m:, :]\n\u001b[32m    350\u001b[39m     activation_logits = torch.matmul(\n\u001b[32m    351\u001b[39m         nn.functional.softmax(\u001b[38;5;28mself\u001b[39m.attention_pool(activation_logits), dim=\u001b[32m1\u001b[39m).transpose(-\u001b[32m1\u001b[39m, -\u001b[32m2\u001b[39m),\n\u001b[32m    352\u001b[39m         activation_logits).squeeze(-\u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\3d-hcct\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\3d-hcct\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\3d-hcct\\model.py:318\u001b[39m, in \u001b[36mEncoder.forward\u001b[39m\u001b[34m(self, x, output_attentions)\u001b[39m\n\u001b[32m    316\u001b[39m all_attentions = []\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m     x, attention_probs = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[32m    320\u001b[39m         all_attentions.append(attention_probs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\3d-hcct\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\3d-hcct\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\3d-hcct\\model.py:289\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, x, output_attentions)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, output_attentions=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    288\u001b[39m     attention_output, attention_probs = \\\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayernorm_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    291\u001b[39m     \u001b[38;5;66;03m# Skip connection\u001b[39;00m\n\u001b[32m    292\u001b[39m     x = x + attention_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\3d-hcct\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\3d-hcct\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\3d-hcct\\model.py:181\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x, output_attentions)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, output_attentions=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     attention_outputs = \u001b[43m[\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheads\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    182\u001b[39m     attention_output = torch.cat([attention_output \u001b[38;5;28;01mfor\u001b[39;00m attention_output, _ \u001b[38;5;129;01min\u001b[39;00m attention_outputs], dim=-\u001b[32m1\u001b[39m)\n\u001b[32m    183\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output_projection(attention_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\3d-hcct\\model.py:181\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, output_attentions=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     attention_outputs = [\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.heads]\n\u001b[32m    182\u001b[39m     attention_output = torch.cat([attention_output \u001b[38;5;28;01mfor\u001b[39;00m attention_output, _ \u001b[38;5;129;01min\u001b[39;00m attention_outputs], dim=-\u001b[32m1\u001b[39m)\n\u001b[32m    183\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output_projection(attention_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\3d-hcct\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\3d-hcct\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\3d-hcct\\model.py:142\u001b[39m, in \u001b[36mAttentionHead.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mvalue:- \u001b[39m\u001b[33m'\u001b[39m, value.shape)\n\u001b[32m    140\u001b[39m \u001b[38;5;66;03m# Calculate the attention scores\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# softmax(Q*K.T/sqrt(head_size))*V\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m attention_scores = torch.matmul(query, \u001b[43mkey\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    143\u001b[39m attention_scores = attention_scores / math.sqrt(\u001b[38;5;28mself\u001b[39m.attention_head_size)\n\u001b[32m    144\u001b[39m attention_probs = nn.functional.softmax(attention_scores, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "Attn = []\n",
    "model.eval()\n",
    "for file in Files:\n",
    "    file_path = os.path.join(DataFolder, file)\n",
    "    img = nib.load(file_path)\n",
    "    x1 = img1.get_fdata() \n",
    "    x1 = torch.from_numpy(x1)\n",
    "    x1 = x1.unsqueeze(0).unsqueeze(0).float()\n",
    "    with torch.no_grad():\n",
    "        logits, all_attentions = model(x1, output_attentions=True)\n",
    "    \n",
    "    attn = torch.stack(all_attentions, dim=0)\n",
    "    attn_mean = attn.mean(dim=0)\n",
    "    avg_attn = attn_mean.mean(dim=1)\n",
    "    avg_attn = avg_attn.mean(dim=0)\n",
    "\n",
    "    Attn.append(avg_attn)\n",
    "    print(avg_attn.shape)\n",
    "    del x1, logits, all_attentions, attn, attn_mean, avg_attn\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "Attn = torch.mean(torch.stack(Attn), dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58c80bd2-4bbd-4955-899d-eb1965e3f63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.666666666666664"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cbb6e1-31bd-4b76-9ee7-f2fa30981b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9241c023-6b35-46b4-8ca1-2c2ec7db9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_mat = Attn\n",
    "residual_att = torch.eye(att_mat.size(1)).to(device=\"cpu\")\n",
    "aug_att_mat = att_mat + residual_att\n",
    "aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "\n",
    "# Recursively multiply the weight matrices\n",
    "joint_attentions = torch.zeros(aug_att_mat.size()).to(device=\"cpu\")\n",
    "joint_attentions[0] = aug_att_mat[0]\n",
    "\n",
    "for n in range(1, aug_att_mat.size(0)):\n",
    "    joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n - 1])\n",
    "\n",
    "# Attention from the output token to the input space.\n",
    "v = joint_attentions[0,1:].to(device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2ab370-3339-4d3b-87a2-963f5bccabd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "print(len(v))\n",
    "# mask = v.reshape(15, 13).detach().numpy()  # Attn1\n",
    "mask = v.reshape(16, 32).detach().numpy()\n",
    "mask = cv2.resize(mask / mask.max(), (109, 109))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af60217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "Attn1 = []\n",
    "# extra_features= torch.randn(1,108)\n",
    "x1 = torch.randn(1, 1, 91, 109, 91)\n",
    "x2 = torch.randn(1, 1, 91, 109, 91)\n",
    "\n",
    "# output= model(x, extra_features)\n",
    "logits1, all_attentions1 = model(x1, output_attentions=True)\n",
    "logits2, all_attentions2 = model(x2, output_attentions=True)\n",
    "print('all_attentions :-  ', len(all_attentions1),'  ',all_attentions1[0].shape)\n",
    "attn1 = torch.stack(all_attentions1, dim=0)\n",
    "attn2 = torch.stack(all_attentions2, dim=0) \n",
    "print('attn1:- ', attn1.shape)\n",
    "\n",
    "attn1_mean = attn1.mean(dim=0)\n",
    "avg_attn1 = attn1_mean.mean(dim=1)\n",
    "avg_attn1 = avg_attn1.mean(dim=0)\n",
    "\n",
    "attn2_mean = attn2.mean(dim=0)\n",
    "avg_attn2 = attn2_mean.mean(dim=1)\n",
    "avg_attn2 = avg_attn2.mean(dim=0)\n",
    "\n",
    "Attn1.append(avg_attn1)\n",
    "Attn1.append(avg_attn2)\n",
    "Attn1 = torch.mean(torch.stack(Attn1), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72919dce-159c-4a38-b6d0-791c252851f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Attn1[0].shape, len(Attn1), avg_attn1.shape, all_attentions2[0].shape, len(all_attentions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f95c124-8f68-4453-bdee-1da5f17e34f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_mat = Attn1\n",
    "residual_att = torch.eye(att_mat.size(1)).to(device=\"cpu\")\n",
    "aug_att_mat = att_mat + residual_att\n",
    "aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "\n",
    "# Recursively multiply the weight matrices\n",
    "joint_attentions = torch.zeros(aug_att_mat.size()).to(device=\"cpu\")\n",
    "joint_attentions[0] = aug_att_mat[0]\n",
    "\n",
    "for n in range(1, aug_att_mat.size(0)):\n",
    "    joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n - 1])\n",
    "\n",
    "# Attention from the output token to the input space.\n",
    "v = joint_attentions[0,1:].to(device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f36186d-0e3f-4a1e-9fa9-606514237508",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_attentions.shape, aug_att_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a4a487-f135-43f6-95cc-93a79f01dccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fb8082-dc85-458f-a422-02504b0094e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "print(len(v))\n",
    "# mask = v.reshape(15, 13).detach().numpy()  # Attn1\n",
    "mask = v.reshape(16, 32).detach().numpy()\n",
    "mask = cv2.resize(mask / mask.max(), (91, 109))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fcf436-dc66-44a7-bd8c-5c1b48bf4ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a15602-3b70-4770-9e70-72f542d11810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example 2D array\n",
    "array2d = mask\n",
    "\n",
    "plt.imshow(array2d, cmap='viridis')  # You can change cmap to 'gray', 'hot', etc.\n",
    "plt.colorbar()  # Adds color scale legend\n",
    "plt.title(\"2D Array Plot\")\n",
    "plt.xlabel(\"X-axis\")\n",
    "plt.ylabel(\"Y-axis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6c1b1f-7bdc-4e47-94db-b98fdc0a91dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example 2D array\n",
    "array2d = mask\n",
    "\n",
    "plt.imshow(array2d, cmap='viridis')  # You can change cmap to 'gray', 'hot', etc.\n",
    "plt.colorbar()  # Adds color scale legend\n",
    "plt.title(\"2D Array Plot\")\n",
    "plt.xlabel(\"X-axis\")\n",
    "plt.ylabel(\"Y-axis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37eda30-b7c0-442b-92cb-c5e5e6919c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2868f76-63e3-4527-89e9-dd4e1b9b16b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43b4c75-56b8-4b64-9b3d-345ab30fd9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e734fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406edfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680502b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca41c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(b[0]), b[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed26145",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(b[1]), b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c667b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(b[2]), b[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f22a34",
   "metadata": {},
   "source": [
    "# GradCam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c472e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eecfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nibabel as nib\n",
    "# import numpy as np\n",
    "\n",
    "# nii_path = r\"D:\\vestibularSchwannoma\\AIIMsInfDataset\\ImageNiftis\\GKPFX2671\\vs_gk_0000.nii.gz\"\n",
    "# img = nib.load(nii_path).get_fdata()\n",
    "# DEVICE= 'cpu'\n",
    "\n",
    "# # Normalize and convert to tensor [B, C, D, H, W]\n",
    "# img = (img - np.mean(img)) / np.std(img)\n",
    "# img_tensor = torch.tensor(img, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "# img_tensor.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0556949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img= torch.randn(91, 109, 91)  \n",
    "img_tensor = torch.randn(1, 1, 91, 109, 91)\n",
    "img_tensor.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0368c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import LayerGradCam\n",
    "\n",
    "# Choose final conv layer in DenseNet201 (last layer in last dense block)\n",
    "target_layer = model.embedding.patch_embeddings.conv_5\n",
    "\n",
    "gradcam = LayerGradCam(model, target_layer)\n",
    "\n",
    "# Get Grad-CAM attribution for regression output\n",
    "attribution = gradcam.attribute(img_tensor, target=0)\n",
    "attr_map = attribution.squeeze().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5751e413",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribution.shape, attr_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daf8f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attribution[0][0]==attr_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cf0cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# attribution shape: [1, 1, D', H', W']\n",
    "# Upsample to match input shape: [1, 1, 130, 130, 130] or whatever your input was\n",
    "upsampled_attr = F.interpolate(\n",
    "    attribution, size=img_tensor.shape[2:], mode='trilinear', align_corners=False\n",
    ").squeeze().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd59d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_attr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f447027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only HeatMap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# raw_img: already shape (D, H, W)\n",
    "attr_map = upsampled_attr\n",
    "\n",
    "# Get center slices\n",
    "z, y, x = [s // 2 for s in attr_map.shape]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(attr_map[z], cmap='hot', alpha=0.5)\n",
    "plt.title(\"Axial\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(attr_map[:, y, :], cmap='hot', alpha=0.5)\n",
    "plt.title(\"Coronal\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(attr_map[:, :, x], cmap='hot', alpha=0.5)\n",
    "plt.title(\"Sagittal\")\n",
    "\n",
    "plt.suptitle(\"Upsampled Grad-CAM on HCCT\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50a5005",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp= upsampled_attr.squeeze()\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286f55b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlayyed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pick center slice for each axis\n",
    "z, y, x = [s // 2 for s in img.shape]\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Axial view\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(img[z], cmap='gray')\n",
    "plt.imshow(upsampled_attr[z], cmap='hot', alpha=0.5)\n",
    "plt.title(\"Axial\")\n",
    "\n",
    "# Coronal view\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(img[:, y, :], cmap='gray')\n",
    "plt.imshow(upsampled_attr[:, y, :], cmap='hot', alpha=0.5)\n",
    "plt.title(\"Coronal\")\n",
    "\n",
    "# Sagittal view\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(img[:, :, x], cmap='gray')\n",
    "plt.imshow(upsampled_attr[:, :, x], cmap='hot', alpha=0.5)\n",
    "plt.title(\"Sagittal\")\n",
    "\n",
    "plt.suptitle(\"Grad-CAM Overlay on Input MRI\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0887eb68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a566e1ea",
   "metadata": {},
   "source": [
    "# Attention Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89ffeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 1, 91, 109, 91)\n",
    "output = model(x, output_attentions=True)\n",
    "a, b = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b395d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(len(b)):\n",
    "\n",
    "    attn_prob= b[i]\n",
    "\n",
    "    # Step 1: Extract CLS token attention to all other tokens\n",
    "    # Shape becomes [1, 8, 512]\n",
    "    cls_attn = attn_prob[:, :, 0, 1:]\n",
    "\n",
    "    # Step 2: Average over attention heads  shape: [1, 512]\n",
    "    cls_attn_mean = cls_attn.mean(dim=1)\n",
    "\n",
    "    # Step 3: Reshape to 3D grid (8 x 8 x 8)\n",
    "    attn_3d = cls_attn_mean.view(1, 1, 8, 8, 8)  # [B, C, D, H, W]\n",
    "\n",
    "    # Step 4: Interpolate to original shape [91, 109, 91]\n",
    "    # Mode: 'trilinear' for 3D\n",
    "    attn_upsampled = F.interpolate(attn_3d, size=(91, 109, 91), mode='trilinear', align_corners=False)\n",
    "\n",
    "    # Step 5: Normalize the attention map for visualization\n",
    "    attn_norm = (attn_upsampled - attn_upsampled.min()) / (attn_upsampled.max() - attn_upsampled.min())\n",
    "\n",
    "    # Step 6: Convert to numpy for visualization or saving\n",
    "    attn_map_np = attn_norm.squeeze().detach().numpy()  # shape: (91, 109, 91)\n",
    "\n",
    "    # Optional: Visualize a middle slice in each plane\n",
    "    print(\"Attention map corresponding to transformer block \", i+1)\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(attn_map_np[45, :, :], cmap='hot')\n",
    "    plt.title(\"Axial slice\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(attn_map_np[:, 54, :], cmap='hot')\n",
    "    plt.title(\"Coronal slice\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(attn_map_np[:, :, 45], cmap='hot')\n",
    "    plt.title(\"Sagittal slice\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # print(attn_prob.shape, cls_attn.shape, cls_attn_mean.shape)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c74662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59c86ca9",
   "metadata": {},
   "source": [
    "# Attention Maps using captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c862b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer = model.encoder.blocks[2].attention.heads[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cb2984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_transform(tensor):\n",
    "    # Inspect `tensor.shape` here; often it is (batch, n_tokens, embed_dim)\n",
    "    # Adapt the reshaping to match your attention head output\n",
    "    # Below is a common pattern, but may need adjustment:\n",
    "    batch_size, num_tokens, embed_dim = tensor.size()\n",
    "    spatial_dim = int(num_tokens ** 0.5)\n",
    "    return tensor.permute(0, 2, 1).reshape(batch_size, embed_dim, spatial_dim, spatial_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bee039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM\n",
    "\n",
    "cam = GradCAM(model=model, target_layers=[target_layer], reshape_transform=reshape_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9e6c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscale_cam = cam(input_tensor=x, targets=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dfa56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-gradcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde4941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import LayerGradCam\n",
    "\n",
    "# Choose final conv layer in DenseNet201 (last layer in last dense block)\n",
    "target_layer = model.encoder.blocks[2].attention.heads[7]\n",
    "\n",
    "gradcam = LayerGradCam(model, target_layer)\n",
    "\n",
    "# Get Grad-CAM attribution for regression output\n",
    "attribution = gradcam.attribute(img_tensor, target=0)\n",
    "# attr_map = attribution.squeeze().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f7026d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
